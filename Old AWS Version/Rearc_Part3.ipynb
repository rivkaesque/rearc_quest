{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ad6ffde-0714-47c8-92a6-ee4e8260e89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "source": [
    "####  Run this cell to set up and start your interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2dee39-0b28-42d6-af70-134c4ed6a670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 5.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "%region us-east-2\n",
    "\n",
    "#using pre-generated standard imports; will hopefully learn which are actually needed in AWS later\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window, types as T\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c7c337-2116-4da7-a0b2-268ce1532624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#setup environment\n",
    "try:\n",
    "    args = getResolvedOptions(sys.argv, ['bucket_name', 'reports_prefix', 'bls_file_path', 'population_file_path'])\n",
    "except:\n",
    "    args = {}\n",
    "\n",
    "BUCKET_NAME = args.get('bucket_name', 'rivkasfirstawsbucket')\n",
    "REPORTS_PREFIX = args.get('reports_prefix', 'reports/')\n",
    "BLS_FILE = args.get('bls_file_path', 'bls_data/raw/pr.data.0.Current')\n",
    "POPULATION_FILE = args.get('population_file_path', 'population_data/raw/population_data.json')\n",
    "print(BUCKET_NAME, REPORTS_PREFIX, BLS_FILE, POPULATION_FILE)\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "spark = SparkSession.builder.appName(\"BLSAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0c28651-db61-4cac-b15c-afa7b2692d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read bls\n",
    "bls_df = spark.read.option(\"header\", True) \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(f\"s3://{BUCKET_NAME}/{BLS_FILE}\")\n",
    "\n",
    "bls_df = bls_df.toDF(*[c.strip() for c in bls_df.columns])\n",
    "\n",
    "#trim string columns\n",
    "string_columns = [field.name for field in bls_df.schema.fields if isinstance(field.dataType, T.StringType)]\n",
    "print(string_columns)\n",
    "for col_name in string_columns:\n",
    "    bls_df = bls_df.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "\n",
    "bls_df.printSchema()\n",
    "bls_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4f3b49-2997-45cc-9bf4-91a6f0a3f7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#aggregate bls data\n",
    "#may need to clean\n",
    "sum_per_year = bls_df.groupBy(\"series_id\", \"year\").agg(F.sum(\"value\").alias(\"yearly_sum\"))\n",
    "\n",
    "\n",
    "best_year = sum_per_year.withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"series_id\").orderBy(F.desc(\"yearly_sum\")))) \\\n",
    "                                   .filter(F.col(\"rank\") == 1) \\\n",
    "                                   .drop(\"rank\")\n",
    "\n",
    "best_year.show()\n",
    "#todo: rework to create tables. Need to learn AWS lakehouse.\n",
    "best_year.toPandas().to_csv(f\"s3://{BUCKET_NAME}/reports/best_year.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fdd58b-637d-4168-b4a4-317cb2d31ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "population_df = spark.read.option(\"multiline\", True).json(f\"s3://{BUCKET_NAME}/{POPULATION_FILE}\")\n",
    "\n",
    "population_df = population_df.select(F.explode(\"data\").alias(\"data\"))\n",
    "population_df = population_df.select(\"data.*\")\n",
    "\n",
    "population_df.printSchema()\n",
    "population_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14841950-3c38-4101-9da4-ebb46e127b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#filter population data and aggregate:\n",
    "#may need to clean\n",
    "population_df = population_df.filter((F.col(\"Year\") >= 2013) & (F.col(\"Year\") <= 2018))\n",
    "\n",
    "population_stats = population_df.agg(\n",
    "    F.mean(\"Population\").alias(\"mean_population\"),\n",
    "    F.stddev(\"Population\").alias(\"std_population\")\n",
    ")\n",
    "\n",
    "population_stats.show()\n",
    "\n",
    "#todo: rework to create tables. Need to learn AWS lakehouse.\n",
    "population_stats.toPandas().to_csv(f\"s3://{BUCKET_NAME}/reports/population_stats.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c76df4-1b56-4e85-82b0-2aebbfd9f9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#join the datasets and filter:\n",
    "target_series = \"PRS30006032\"\n",
    "target_period = \"Q01\"\n",
    "\n",
    "report = bls_df.filter((F.col(\"series_id\") == target_series) & (F.col(\"period\") == target_period))\\\n",
    "                .join(population_df, bls_df.year == population_df.Year, how=\"left\")\\\n",
    "                .select(bls_df[\"*\"], \"Population\")\n",
    "\n",
    "\n",
    "\n",
    "report.show()\n",
    "\n",
    "#todo: rework to create tables. Need to learn AWS lakehouse.\n",
    "report.toPandas().to_csv(f\"s3://{BUCKET_NAME}/reports/bls_with_population.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Rearc_Part3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
